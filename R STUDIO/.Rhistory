library(ggrepel)
library(ggsci) # scientific colors
library(viridis) #looks good and includes many options
library(RColorBrewer) # wide choice
library(countrycode) # Country name tools
mygdx <- gdx("results_ssp2_bau.gdx")
var_names = mygdx$variables$name
par_names = mygdx$parameters$name
sets_names = mygdx$sets$name
temp_usa <- subset(mygdx["TEMP"],
n == "usa" & m == "atm",
select = c("t", "value"))
temp_usa
plot(temp_usa,lwd=2,type = "l")
mygdx["n"][[1]]
mygdx["n"]
mygdx["wemi"]
mygdx["wemi"] %>% as_tibble() # removes row names and each colum is a list?
print(attr(mygdx["wemi"],"gams"))
mygdx["Q_EMI"] %>% as_tibble()
# to load upper bound
mygdx["Q_EMI",field = "up"] %>% as_tibble()
myfiles = file.path('C:/Users/modin/Desktop/Ettore/UNIVERSITA/ECC_GAMS/R STUDIO',c("results_ssp2_bau.gdx","results_ssp2_ctax50.gdx"))
qemi = batch_extract("Q_EMI",files = myfiles)[[1]]
qemi %>% as_tibble()
#mygdx["TEMP"]$value
dice_csv <- read_csv('Dice2016R-091916ap.csv', skip = 6) # returns already a tibble
# rename Year to Variable
dice_csv <- dice_csv%>%rename(Variable=Year)
dice_csv
# let's make a long table we will need it for plotting
mdat_2c = dice_csv %>% pivot_longer(!Variable, names_to = "Year")
# Year should be numerical
mdat_2c <- mdat_2c %>% mutate(Year = as.numeric(Year))
mdat_2c
ggplot(data = mdat_2c %>% filter(Variable == 'Consumption'),
mapping = aes(x = Year,y = value)) +
geom_line() +
labs(title = "Consumption - DICE - 2C Scenario", x = "", y = "Consumption [trillion USD]") +
theme_bw() +
scale_y_continuous(expand = c(0,0), limits = c(0,NA)) # Includes 0 in the y-axis, NA takes the max in the dataset
# + scale_x_continuous(expand = c(0,0), limits = c(2020,2320)) # Zoom-in in the x-axis
vars = c('Consumption','Atmospheric Temperature',
'Industrial Emissions GTCO2 per year',
'damages')
p<-ggplot(mdat_2c %>% filter(Variable %in% vars),
aes(x = Year, y = value)) +
geom_line() +
labs(x = "", y = "") +
facet_wrap(~Variable, ncol = 4, scales='free') + # - free scales
theme_bw()
p
#ggsave(filename = "Fig1.pdf", plot = p, width = 21, height = 29.7, units = "cm") # A4 format
#ggsave(filename = "Fig1.png", plot = p, width = 8, height = 5) # smaller png file
# read several GDX (several scenarios with different carbon taxes)
myfiles <- file.path("C:/Users/modin/Desktop/Ettore/UNIVERSITA/ECC_GAMS/R STUDIO", c("results_ssp2_bau.gdx","results_ssp2_ctax50.gdx","results_ssp2_ctax100.gdx"))
res <- batch_extract("Q_EMI", files = myfiles) # load GHG emissions
# have a look at the structure
str(res) # a list of a data frame
# this grabs the data.frame inside the list GHG$Q_EMI
qemi = res$Q_EMI %>% as_tibble()
# The element in a list can also be accessed with res[[1]] or res[['Q_EMI']]
qemi
qemi_brazil = qemi[which(qemi$n=="brazil"),]
qemi_brazil
qemi <- qemi %>%
mutate(year = as.numeric(t) * 5 + 2000) %>% # Transform time period into year
mutate(scen = basename(gdx)) %>% # Transform the filename into a scenario name
mutate(scen = str_replace(scen, ".gdx", "")) %>%
mutate(scen = str_replace(scen, "results_ssp2_", ""))
# basename returns the filename without the path
# str_replace replace a given string pattern
qemi <- qemi %>%
mutate(scen = factor(scen, levels = unique(qemi$scen)))
qemi
# group_by scenarious, emission and year and summurise other all countrys using sum
wemi <- qemi %>%
group_by(scen,e,year) %>%
summarise(value = sum(value))
wemi
# Let's change unit from GtC to GtCO2
wemi <- wemi %>%
mutate(value = value * 44 / 12)
ggplot(wemi %>% filter(e == 'co2' & year <= 2100),
aes(x = year, y = value, color = scen)) +
geom_line() +
geom_point() + # NEW GEOMETRY, SAME DATA AND MAPPING
#geom_point(data = wemi %>% filter(e = 'co2' & year = 2100 & scen = "ctax100")) + #SUBSET BUT SAME MAP
geom_hline(yintercept = 0, color = 'Black', size = 0.8) +
labs(x = "", y = "CO2 [GtC]", title = "CO2 emissions") +
theme_bw() +
scale_x_continuous(limits=c(2005, 2100),
breaks=seq(2005, 2100,by = 10), # Define the axis tiks breaks
guide = guide_axis(n.dodge = 2)) + # avoid overlapping
scale_y_continuous(limits=c(-5, 50)) +
#scale_color_discrete(name = "Scenario")
scale_color_viridis(discrete = TRUE, option = "D") # use "?" to check the options
pop <- batch_extract("l",files = myfiles)[[1]] %>% as_tibble() # Load population
# let's transform time periods into Years
pop <- pop %>%
mutate(year = as.numeric(t) * 5 + 2000) %>% # Transform time period into year
mutate(scen = basename(gdx)) %>% # Transform the filename into a scenario name
mutate(scen = str_replace(scen, ".gdx", "")) %>%
mutate(scen = str_replace(scen, "results_ssp2_", ""))
# let's keep 2 emission parameters
dat = full_join(qemi %>% filter(e %in% c('kghg','ccs')) %>% select(-gdx,-t),
pop %>% select(-gdx,-t),
by = c("scen","year","n"),
suffix = c(".ghg",".pop"))
# have a look, it's magical
dat
ggplot(dat %>% filter(year < 2100 & e == 'kghg'),
aes(x = value.pop, y = value.ghg, color = n)) +
geom_point()
ggplot(dat %>% filter(year < 2100 & e == 'kghg'),
aes(x = value.pop,y = value.ghg, color = n,
sahpe = scen,    # add shape
alpha = year,    # add transparency
size = e)) +     # add size with 'e'
geom_hline(yintercept = 0) + # add a horizontal line at 0
geom_vline(xintercept = 1000, color = 'red') + # Separate low-high population regions
geom_point() +
geom_point(data = dat %>% filter(year == 2050 & value.pop > 1000 & e == 'kghg'),
color='green')+ # Emphasize a few points
geom_label_repel(aes(label = n),
dat %>% filter(year == 2050 & value.pop > 1000 & e == 'kghg'),
color = 'black', alpha = 1) + # Add labels
scale_x_continuous(name = "Population [millions]",expand = c(0.1, 0.1)) +
scale_y_continuous(name = "CO2 [GtCe]",expand = c(0.1, 0.1))+
scale_color_viridis(discrete = TRUE, option = "B", name = "Region") +
facet_grid(e~scen, scales='free') + # Add facets
guides(col = guide_legend(ncol = 9)) + # Legend in 9 columns
#theme_bw() +
theme(legend.position = "bottom") # Legend in the bottom
# Update the scenario as factors (ordered set)
dat <- dat %>% mutate(scen = factor(scen, levels = c('bau','ctax50','ctax100')))
ggplot(dat %>% filter(e == 'kghg'), aes(x = scen, y = value.ghg)) +
geom_boxplot(outlier.shape = NA) + theme_bw() +
geom_hline(yintercept = 0, color = "red") +
geom_jitter(aes(color = n), shape = 16,
position = position_jitter(0.2), alpha = 0.6) + # Add observations
stat_summary(fun = mean, geom = "point", shape = 16, size = 4) + # Add mean distribution
labs(x = "Scenario", y = "GHG [GtCe]") +
scale_color_viridis(name="Regions",discrete = TRUE, option = "D")+
guides(col = guide_legend(ncol = 3))# color per region
# import the WITCH model regional definition
wreg <- read_csv('mapwitch17.csv')
# transforms ISO codes into country names
wreg <- wreg %>% mutate(region = countrycode(ISO, origin = 'iso3c', destination = 'country.name'))
# Correcting some mismatches
#wreg[ISO ='GBR']$region = 'UK'
#wreg[ISO ='USA']$region = 'USA'
wreg
world <- ne_countries(scale = "small", returnclass = "sf")
world <- subset(world,!adm0_a3 %in% c("ATA","FJI"))
# merge the WITCH regional definition with the world map
world <- merge(world,wreg, by.x = "adm0_a3", by.y = "ISO")
# merge with the emission data.frame
world0 <- merge(world,qemi %>% filter(year == 2100 & e == "co2"), by = "n", allow.cartesian=TRUE)
# Use a better projection 'equal projection'
target_crs <- '+proj=eqearth +wktext'
world1 <- st_transform(world0, crs = target_crs)
ggplot(data = world1 %>% filter(scen == "ctax50")) +
geom_sf(aes(fill = value)) +
coord_sf(datum = target_crs, expand = FALSE, clip = "off") +
scale_fill_distiller(name = "CO2 Emissions\nin 2100", palette ="RdYlGn",direction=-1) +
theme_void() +
theme(legend.position = "bottom",
strip.text.x = element_text(size = 12, face="bold"))+
facet_grid(~scen)
# Put the scenarios values in columns
dqemi <- qemi %>%
select(-t,-gdx) %>%
filter(e == "co2" & year == 2100) %>%
pivot_wider(names_from = "scen", values_from = "value")
# Let's create a new variable/column
dqemi <- dqemi %>% mutate(dif_wrtBAU = bau - ctax100)
# merge with the emission data.frame
world2 <- merge(world,dqemi, by = "n", allow.cartesian=TRUE)
# Use a better projection 'equal projection'
target_crs <- '+proj=eqearth +wktext'
world3 <- st_transform(world2, crs = target_crs)
ggplot(data = world3) +
geom_sf(aes(fill = dif_wrtBAU * 44 / 22)) +
coord_sf(datum = target_crs, expand = FALSE, clip = "off") +
scale_fill_distiller(name = "CO2 emission\nreduction\n[GtCO2]",
palette ="RdYlGn", direction = -1) +
theme_void()
ggplot(data = world1) +
geom_sf(aes(fill = value)) +
coord_sf(datum = target_crs, expand = FALSE, clip = "off") +
scale_fill_distiller(name = "CO2 Emissions\nin 2100", palette ="RdYlGn",direction=-1) +
theme_void() +
theme(legend.position = "bottom",
strip.text.x = element_text(size = 12, face="bold"))+
facet_grid(~scen)
CI
#directories
working_dir ="C:/Users/modin/Desktop/Ettore/UNIVERSITA/PISA_PROJECT/progetto-applied"
dataset_dir = "../data"
include_dir = paste(working_dir,"/src/include/Utilities.R",sep="")
#including utilities
#including utilities
source(include_dir)
#importing the dataset
pisa_data <- read.csv(file=paste(working_dir,"/data/pisa-woNA_school_final.csv",sep=""))
data<-pisa_data
dim(data)
colnames(data)
library(MVN)
library(car)
library(heplots)
library(mvnormtest)
#select only ICT moments fot the moment
library(dplyr)
data <- data %>% dplyr::select(all_of(c("CNT",stu_PROF)))
# what column identify the group?
lab_index <- 1
p = dim(data)[2]
p
v = c()
# Re-name
colnames(data)[lab_index]="label"
head(data)
cols <- seq(1,p)
feats <- cols[!cols %in% lab_index]
data.feats = data[,feats]
data$label<-as.factor(data$label)
p     = length(feats) #don't count the column of labels
# Dimensions
n     = length(data$label) # total number of obs.
ng    = table(data$label)  # number of obs. in each group
treat_factors <- factor(data$label)
treat = levels(treat_factors) # levels of the treatment
g     = length(treat)      # number of levels (i.e., of groups)
indeces = list()
for (jj in 1:g){
indeces[jj] <- list(which(data$label == treat[jj]))
}
n_list = list()
for(j in 1:g) {
n_list[j] = list(length(indeces[[j]]))
}
n=0
for(j in 1:g) {
n = n +  n_list[[j]]
}
# Plot: different panels -> different group
#x11()
par(mfrow=c(2,g/4))
for (j in 1:(g/2)){
boxplot(data.feats[as.numeric(indeces[[j]]),], main=paste('group ',j),  ylim=c(round(min(data.feats)),
round(max(data.feats))), col = rainbow(g)[j])
}
#x11()
par(mfrow=c(4,g/8))
for (j in (g/2+1):g){
boxplot(data.feats[as.numeric(indeces[[j]]),], main=paste('group ',j),  ylim=c(round(min(data.feats)),
round(max(data.feats))), col = rainbow(g)[j])
}
CI
fit = manova(as.matrix(data.feats) ~ data$label)
summary.manova(fit, test="Wilks")
# Pr(>F) = p-value of H0 vs. H1
# If it is very small -> H1 -> the treatment was effective
# Comment
# If p<=2 and g<=3 we have an exact test (with Wilks)
Ps_mvn <- NULL
for(j in 1:g) {
mvn.test <- mvn(data = data[indeces[[j]] , feats])
Ps_mvn <- c(Ps_mvn, mvn.test$multivariateNormality$`p value`)
}
Ps_mvn
# 2. same covariance structure (homoschedasticity)
S  = cov(data.feats)
Slist = list()
for (j in 1:g){
Slist[j] = list(cov(data.feats[indeces[[j]],]))
# Qualitatively:
round(Slist[[j]],digits=1)
}
summary(boxM(data.feats, treat_factors))
temp <- do.call(rbind, Slist)
for (j in 1:g){
image(Slist[[j]], col=heat.colors(100),main=paste('Cov. S',j), asp=1, axes = FALSE,
breaks = quantile(temp, (0:100)/100, na.rm=TRUE))
}
# First of all:
#   Let's see on which variables the group has an effect.
#   Via ANOVA: for each feature we perform an ANOVA to see if the belonging to
#   the group has an effect on the mean of the variables.
summary.aov(fit,test="Wilks")
# Comment
# Pr(>F) = p-value small -> the group has an influence on that X_k
# This analysis does NOT say either which groups differ nor which are the variables
# for which the groups differ.
alpha = 0.0001
k     = p*g*(g-1)/2
qT    = qt(1-alpha/(2*k), n-g)
W1 <- diag(t(fit$res) %*% fit$res) # fit$re^2
SSres <- sum(W1)
var = SSres/(n-g)
W     = summary.manova(fit)$SS$Residuals
SSres == sum(diag(W))
m  = sapply(data.feats,mean)      # estimates mu
m_list = list()
for (j in 1:g){
m_list[j] = list(sapply(data.feats[indeces[[j]],],mean)) # estimates mu.1=mu+tau.j
}
mg <- do.call(rbind, m_list)
mg
inf_list = list()
sup_list = list()
mean_list = list()
## be careful, order could be reversed
for(i in 1:(g-1)) {
for(j in (i+1):g) {
ind <- (i-1)*g-i*(i-1)/2+(j-i)
inf_list[ind] <- list(m_list[[i]]- m_list[[j]] - qT * sqrt( diag(W)/(n-g)
*(1/n_list[[i]]+1/n_list[[j]])))
mean_list[ind]<-list(m_list[[i]]- m_list[[j]])
sup_list[ind] <- list(m_list[[i]]- m_list[[j]] + qT * sqrt( diag(W)/(n-g)
*(1/n_list[[i]]+1/n_list[[j]])))
}
}
CI = list()
for(i in 1:ind){
CI[i] <- list(cbind(inf = inf_list[[i]],mean = mean_list[[i]], sup = sup_list[[i]]))
}
names_CI<-c()
for(i in 1:(g-1)) {
for(j in (i+1):g) {
names_CI<-c(names_CI,paste(treat[i],treat[j]))
}
}
names(CI)<-names_CI
CI
# Chunk 1
# working_dir ="C:/Users/modin/Desktop/Ettore/UNIVERSITA/PISA_PROJECT/progetto-applied"
# dataset_dir = "../data"
## dovrebbe funzionare anche senza dare il percorso completo ma usando solo questo:
dataset_dir = "../../data/"
## cioÃ¨ usaando la posizione relativa dal file
## perchÃ© vuol dire vai due cartelle indietro da qui, e poi in data
## e quindi anche qui
# include_dir = paste(working_dir,"/src/include/Utilities.R",sep="")
include_dir = "../include/Utilities.R"
#including utilities
source(include_dir)
# working_dir ="C:/Users/modin/Desktop/Ettore/UNIVERSITA/PISA_PROJECT/progetto-applied"
# dataset_dir = "../data"
## dovrebbe funzionare anche senza dare il percorso completo ma usando solo questo:
dataset_dir = "../../data/"
## cioÃ¨ usaando la posizione relativa dal file
## perchÃ© vuol dire vai due cartelle indietro da qui, e poi in data
## e quindi anche qui
# include_dir = paste(working_dir,"/src/include/Utilities.R",sep="")
include_dir = "../include/Utilities.R"
#including utilities
source(include_dir)
#variabili finite nel dataset
tec=c("ICTCLASS","ICTHOME","ICTOUTSIDE","ICTRES","AUTICT","COMPICT","INTICT","ENTUSE","HOMESCH","USESCH")
psi=c("ATTLNACT","EMOSUPS","COMPETE","EUDMO","GFOFAIL","SWBP","RESILIENCE","BELONG","BEINGBULLIED","PERFEED")
clt=c("JOYREAD","CULTPOSS","HEDRES","SCREADCOMP","LMINS","MMINS")
fam=c("WEALTH","ESCS","HOMEPOS","BFMJ2","BMMJ1","HISCED","HISEI")
tch=c("TEACHINT","TEACHSUP","STIMREAD")
sch=c("PERCOMP","PERCOOP","ICTSCH","RATCMP1") #,"RATCMP2")
variabole_groups = list(tec,psi,clt,fam,sch)
G = length(variabole_groups)
n = dim(pisa_data)[1]
p = dim(pisa_data)[2]
# all toghether
pisa.fa <- factanal(pisa_data[,c(4:p)], factors = 5, rotation = "varimax")
pisa.fa
pisa.pca <-princomp(pisa_data[,4:p])
pisa.fa$loadings
pisa.load = pisa.pca$loadings
x11()
par(mfcol=c(3,1))
for(i in 1:2) barplot(pisa.load[,i] ,las=2,ylim = c(-1, 1), main=paste("PC Loadings",i))
x11()
par(mfcol=c(3,1))
for(i in 3:4:5) barplot(pisa.load[,i] ,las=2,ylim = c(-1, 1), main=paste("PC Loadings",i))
n = dim(pisa_data)[1]
p = dim(pisa_data)[2]
# all toghether
pisa.fa <- factanal(pisa_data[,c(4:p)], factors = 7, rotation = "varimax")
n = dim(pisa_data)[1]
p = dim(pisa_data)[2]
# all toghether
pisa.fa <- factanal(pisa_data[,c(4:p)], factors = 6, rotation = "varimax")
n = dim(pisa_data)[1]
p = dim(pisa_data)[2]
# all toghether
pisa.fa <- factanal(pisa_data[,c(4:p)], factors = 5, rotation = "varimax")
pisa.fa
pisa.pca <-princomp(pisa_data[,4:p])
pisa.fa$loadings
pisa.load = pisa.pca$loadings
x11()
par(mfcol=c(3,1))
for(i in 1:2) barplot(pisa.load[,i] ,las=2,ylim = c(-1, 1), main=paste("PC Loadings",i))
x11()
par(mfcol=c(3,1))
for(i in 3:4:5) barplot(pisa.load[,i] ,las=2,ylim = c(-1, 1), main=paste("PC Loadings",i))
#directories
#working_dir = "/Users/marcogalliani/Desktop/PISA-dev-local"
working_dir ="C:/Users/modin/Desktop/Ettore/UNIVERSITA/PISA_PROJECT/progetto-applied"
dataset_dir = "../data"
include_dir = paste(working_dir,"/src/include/Utilities.R",sep="")
#including utilities
source(include_dir)
#importing the dataset
pisa_data <- read.csv(file=paste(working_dir,"/data/pisa-woNA_school_final.csv",sep=""))
dim(pisa_data)
colnames(pisa_data)
#select only ICT moments fot the moment
library(dplyr)
#pisa_data <- pisa_data %>% dplyr::select(all_of(c("CNT",stu_ICT,"RATCMP1")))
image(cov(select_if(pisa_data,is.numeric)))
pisa_data$RATCMP1 <- NULL #RATCMP2 seems has null estimated variance and gives problems
#image(cov(select_if(pisa_data,is.numeric)))
qq_plot_gaussianity <- function(data){
# Compute the quantiles of the data
quantiles <- quantile(data, probs = seq(0, 1, by = 0.01))
# Compute the theoretical quantiles of a normal distribution
norm_quantiles <- qnorm(seq(0, 1, by = 0.01), mean = mean(data), sd = sd(data))
# Create the QQ-plot
plot(norm_quantiles, quantiles, main = "QQ-plot", xlab = "Theoretical quantiles", ylab = "Sample quantiles")
abline(0, 1, col = "red")  # add a reference line
}
# Call the QQ-plot function with the dataset
for(i in 4:dim(pisa_data)[2]){
qq_plot_gaussianity(pisa_data[,i])
}
M <- colMeans(select_if(pisa_data,is.numeric))
S <- cov(select_if(pisa_data,is.numeric))
#computing mahlanobis distance
d2 <- matrix(mahalanobis(select_if(pisa_data,is.numeric), M, S))
image(S)
pisa_data
pisa_data$schID<-as.factor(pisa_data$schID)
M <- colMeans(select_if(pisa_data,is.numeric))
M <- colMeans(select_if(pisa_data,is.numeric))
S <- cov(select_if(pisa_data,is.numeric))
image(S)
#computing mahlanobis distance
d2 <- matrix(mahalanobis(select_if(pisa_data,is.numeric), M, S))
#removing over a certain distance from the mean
pisa_wo_outliers<- pisa_data[which(d2 <= 8), ] #too many observations removed with 8
dim(pisa_data)
dim(pisa_wo_outliers)
#testing again on the dataset wo outliers
result_wo_ouliers <- mvn(data = pisa_wo_outliers, subset = "CNT", mvnTest = "hz",univariateTest = "AD")
dim(pisa_wo_outliers)
dataset <- read.csv(file="curve.csv")
head(dataset)
library(tidyverse)
dataset<- dataset %>% pivot_longer(cols=!Region)
head(dataset)
dataset <- read.csv(file="curve.csv")
head(dataset)
colnames(dataset)<-c("deg0,deg4")
dataset<- dataset[2:4] %>% pivot_longer(cols=!Region)
head(dataset)
dataset<- dataset[,2:4] %>% pivot_longer(cols=!Region)
dataset <- read.csv(file="curve.csv")
head(dataset)
colnames(dataset)<-c("deg0,deg4")
head(dataset)
dataset <- read.csv(file="curve.csv")
head(dataset)
colnames(dataset)<-c("deg0","deg4")
head(dataset)
dataset <- read.csv(file="curve.csv")
head(dataset)
colnames(dataset)<-c("X","Region","deg0","deg4")
head(dataset)
dataset<- dataset[,2:4] %>% pivot_longer(cols=!Region)
head(dataset)
library(gdxtools)
write.gdx("data_mod_lithium.gdx",list(dataset), compress=F)
head(dataset)
str(dataset)
help("write.gdx")
write.gdx("data_mod_lithium.gdx",list(dataset=dataset), compress=F)
dataset <- read.csv(file="curve.csv")
head(dataset)
colnames(dataset)<-c("X","n","deg0","deg4")
dataset<- dataset[,2:4] %>% pivot_longer(cols=!Region)
dataset <- read.csv(file="curve.csv")
head(dataset)
colnames(dataset)<-c("X","n","deg0","deg4")
dataset<- dataset[,2:4] %>% pivot_longer(cols=!Region)
dataset
dataset<- dataset[,2:4] %>% pivot_longer(cols=!n)
head(dataset)
mod:lit <- data.frame(dataset[2],dataset[1],dataset[3])
mod_lit <- data.frame(dataset[2],dataset[1],dataset[3])
head(mod_lit)
write.gdx("data_mod_lithium.gdx",list(trade_poly_lit=mod_lit), compress=F)
dataset<- dataset[,2:4] %>% pivot_longer(cols=!n)
dataset <- read.csv(file="curve.csv")
dataset <- read.csv(file="curve.csv")
dataset <- read.csv(file="curve2.csv")
head(dataset)
colnames(dataset)<-c("X","n","deg0","deg4")
dataset<- dataset[,2:4] %>% pivot_longer(cols=!n)
head(dataset)
mod_lit <- data.frame(dataset[2],dataset[1],dataset[3])
head(mod_lit)
write.gdx("data_mod_lithium.gdx",list(trade_poly_lit=mod_lit), compress=F)
